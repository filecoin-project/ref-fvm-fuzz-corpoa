LLVMFuzzerInitialize:
   79|      1|pub fn initialize(_argc: *const isize, _argv: *const *const *const u8) -> isize {
   80|      1|    // Registers a panic hook that aborts the process before unwinding.
   81|      1|    // It is useful to abort before unwinding so that the fuzzer will then be
   82|      1|    // able to analyse the process stack frames to tell different bugs appart.
   83|      1|    //
   84|      1|    // HACK / FIXME: it would be better to use `-C panic=abort` but it's currently
   85|      1|    // impossible to build code using compiler plugins with this flag.
   86|      1|    // We will be able to remove this code when
   87|      1|    // https://github.com/rust-lang/cargo/issues/5423 is fixed.
   88|      1|    let default_hook = ::std::panic::take_hook();
   89|      1|    ::std::panic::set_hook(Box::new(move |panic_info| {
   90|       |        default_hook(panic_info);
   91|       |        ::std::process::abort();
   92|      1|    }));
   93|       |
   94|       |    // Initialize the `RUST_LIBFUZZER_DEBUG_PATH` cell with the path so it can be
   95|       |    // reused with little overhead.
   96|      1|    if let Ok(path) = std::env::var("RUST_LIBFUZZER_DEBUG_PATH") {
   97|      0|        RUST_LIBFUZZER_DEBUG_PATH
   98|      0|            .set(path)
   99|      0|            .expect("Since this is initialize it is only called once so can never fail");
  100|      1|    }
  101|      1|    0
  102|      1|}

_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now3tsc:
  270|   392k|fn tsc() -> u64 {
  271|   392k|    #[cfg(target_arch = "x86")]
  272|   392k|    use core::arch::x86::_rdtsc;
  273|   392k|    #[cfg(target_arch = "x86_64")]
  274|   392k|    use core::arch::x86_64::_rdtsc;
  275|   392k|
  276|   392k|    unsafe { _rdtsc() }
  277|   392k|}
_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now13is_tsc_stable:
  182|      1|fn is_tsc_stable() -> bool {
  183|      1|    let clock_source =
  184|      1|        read_to_string("/sys/devices/system/clocksource/clocksource0/available_clocksource");
  185|      1|
  186|      1|    clock_source.map(|s| s.contains("tsc")).unwrap_or(false)
  187|      1|}
_RNCNvNtCsiNIzhTJeOBI_8minstant7tsc_now13is_tsc_stable0B5_:
  186|      1|    clock_source.map(|s| s.contains("tsc")).unwrap_or(false)
_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now18monotonic_with_tsc:
  265|   392k|fn monotonic_with_tsc() -> (Instant, u64) {
  266|   392k|    (Instant::now(), tsc())
  267|   392k|}
_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now14cycles_per_sec:
  224|      1|fn cycles_per_sec(anchor: Instant) -> (u64, u64) {
  225|      1|    let (cps, last_monotonic, last_tsc) = _cycles_per_sec();
  226|      1|    let nanos_from_anchor = (last_monotonic - anchor).as_nanos();
  227|      1|    let cycles_flied = cps as f64 * nanos_from_anchor as f64 / 1_000_000_000.0;
  228|      1|    let cycles_from_anchor = last_tsc - cycles_flied.ceil() as u64;
  229|      1|
  230|      1|    (cps, cycles_from_anchor)
  231|      1|}
_RNvMs_NtCsiNIzhTJeOBI_8minstant7tsc_nowNtB4_8TSCLevel17cycles_per_second:
  166|      1|    fn cycles_per_second(&self) -> u64 {
  167|      1|        match self {
  168|       |            TSCLevel::Stable {
  169|      1|                cycles_per_second, ..
  170|      1|            } => *cycles_per_second,
  171|       |            TSCLevel::PerCPUStable {
  172|      0|                cycles_per_second, ..
  173|      0|            } => *cycles_per_second,
  174|      0|            TSCLevel::Unstable => panic!("tsc is unstable"),
  175|       |        }
  176|      1|    }
_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now15__cycles_per_sec:
  234|      1|fn _cycles_per_sec() -> (u64, Instant, u64) {
  235|      1|    let mut cycles_per_sec;
  236|      1|    let mut last_monotonic;
  237|      1|    let mut last_tsc;
  238|      1|    let mut old_cycles = 0.0;
  239|       |
  240|       |    loop {
  241|      2|        let (t1, tsc1) = monotonic_with_tsc();
  242|   392k|        loop {
  243|   392k|            let (t2, tsc2) = monotonic_with_tsc();
  244|   392k|            last_monotonic = t2;
  245|   392k|            last_tsc = tsc2;
  246|   392k|            let elapsed_nanos = (t2 - t1).as_nanos();
  247|   392k|            if elapsed_nanos > 10_000_000 {
  248|      2|                cycles_per_sec = (tsc2 - tsc1) as f64 * 1_000_000_000.0 / elapsed_nanos as f64;
  249|      2|                break;
  250|   392k|            }
  251|       |        }
  252|      2|        let delta = f64::abs(cycles_per_sec - old_cycles);
  253|      2|        if delta / cycles_per_sec < 0.00001 {
  254|      1|            break;
  255|      1|        }
  256|      1|        old_cycles = cycles_per_sec;
  257|       |    }
  258|       |
  259|      1|    (cycles_per_sec.round() as u64, last_monotonic, last_tsc)
  260|      1|}
_RNvMs_NtCsiNIzhTJeOBI_8minstant7tsc_nowNtB4_8TSCLevel3get:
   84|      1|    fn get() -> TSCLevel {
   85|      1|        let anchor = Instant::now();
   86|      1|        if is_tsc_stable() {
   87|      1|            let (cps, cfa) = cycles_per_sec(anchor);
   88|      1|            return TSCLevel::Stable {
   89|      1|                cycles_per_second: cps,
   90|      1|                cycles_from_anchor: cfa,
   91|      1|            };
   92|      0|        }
   93|      0|
   94|      0|        if is_tsc_percpu_stable() {
   95|       |            // Retrieve the IDs of all active CPUs.
   96|      0|            let cpuids = if let Ok(cpuids) = available_cpus() {
   97|      0|                if cpuids.is_empty() {
   98|      0|                    return TSCLevel::Unstable;
   99|      0|                }
  100|      0|
  101|      0|                cpuids
  102|       |            } else {
  103|      0|                return TSCLevel::Unstable;
  104|       |            };
  105|       |
  106|      0|            let max_cpu_id = *cpuids.iter().max().unwrap();
  107|      0|
  108|      0|            // Spread the threads to all CPUs and calculate
  109|      0|            // cycles from anchor separately
  110|      0|            let handles = cpuids.into_iter().map(|id| {
  111|       |                std::thread::spawn(move || {
  112|       |                    set_affinity(id).unwrap();
  113|       |
  114|       |                    // check if cpu id matches IA32_TSC_AUX
  115|       |                    let (_, cpuid) = tsc_with_cpuid();
  116|       |                    assert_eq!(cpuid, id);
  117|       |
  118|       |                    let (cps, cfa) = cycles_per_sec(anchor);
  119|       |
  120|       |                    (id, cps, cfa)
  121|       |                })
  122|      0|            });
  123|      0|
  124|      0|            // Block and wait for all threads finished
  125|      0|            let results = handles.map(|h| h.join()).collect::<Result<Vec<_>, _>>();
  126|       |
  127|      0|            let results = if let Ok(results) = results {
  128|      0|                results
  129|       |            } else {
  130|      0|                return TSCLevel::Unstable;
  131|       |            };
  132|       |
  133|       |            // Indexed by CPU ID
  134|      0|            let mut cycles_from_anchor = vec![0; max_cpu_id + 1];
  135|      0|
  136|      0|            // Rates of TSCs on different CPUs won't be a big gap
  137|      0|            // or it's unstable.
  138|      0|            let mut max_cps = std::u64::MIN;
  139|      0|            let mut min_cps = std::u64::MAX;
  140|      0|            let mut sum_cps = 0;
  141|      0|            let len = results.len();
  142|      0|            for (cpuid, cps, cfa) in results {
  143|      0|                if cps > max_cps {
  144|      0|                    max_cps = cps;
  145|      0|                }
  146|      0|                if cps < min_cps {
  147|      0|                    min_cps = cps;
  148|      0|                }
  149|      0|                sum_cps += cps;
  150|      0|                cycles_from_anchor[cpuid] = cfa;
  151|       |            }
  152|      0|            if (max_cps - min_cps) as f64 / min_cps as f64 > 0.0005 {
  153|      0|                return TSCLevel::Unstable;
  154|      0|            }
  155|      0|
  156|      0|            return TSCLevel::PerCPUStable {
  157|      0|                cycles_per_second: sum_cps / len as u64,
  158|      0|                cycles_from_anchor,
  159|      0|            };
  160|      0|        }
  161|      0|
  162|      0|        TSCLevel::Unstable
  163|      1|    }
_RNvNtCsiNIzhTJeOBI_8minstant7tsc_now4init:
   29|      1|unsafe fn init() {
   30|      1|    let tsc_level = TSCLevel::get();
   31|      1|    let is_tsc_available = match &tsc_level {
   32|      1|        TSCLevel::Stable { .. } => true,
   33|      0|        TSCLevel::PerCPUStable { .. } => true,
   34|      0|        TSCLevel::Unstable => false,
   35|       |    };
   36|      1|    if is_tsc_available {
   37|      1|        *TSC_STATE.nanos_per_cycle.get() = 1_000_000_000.0 / tsc_level.cycles_per_second() as f64;
   38|      1|    }
   39|      1|    *TSC_STATE.is_tsc_available.get() = is_tsc_available;
   40|      1|    *TSC_STATE.tsc_level.get() = tsc_level;
   41|      1|    std::sync::atomic::fence(std::sync::atomic::Ordering::SeqCst);
   42|      1|}
_RNvNvNtCsiNIzhTJeOBI_8minstant7tsc_now23init___rust_ctor___ctor23init___rust_ctor___ctor:
   28|      1|#[ctor::ctor]

